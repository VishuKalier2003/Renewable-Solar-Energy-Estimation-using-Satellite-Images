{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import numpy as np\n",
    "import torch.utils.data\n",
    "import cv2\n",
    "import torchvision.models.segmentation\n",
    "import torch\n",
    "import os\n",
    "batchSize=2\n",
    "imageSize=[600,600]\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')   # train on the GPU or on the CPU, if a GPU is not available\n",
    "trainDir=\"/home/breakeroftime/Documents/Datasets/LabPics/LabPicsChemistry/Train\"\n",
    "\n",
    "imgs=[]\n",
    "for pth in os.listdir(trainDir):\n",
    "    imgs .append(trainDir+\"/\"+pth +\"//\")\n",
    "    \n",
    "def loadData():\n",
    "    batch_Imgs=[]\n",
    "    batch_Data=[]# load images and masks\n",
    "    for i in range(batchSize):\n",
    "        idx=random.randint(0,len(imgs)-1)\n",
    "        img = cv2.imread(os.path.join(imgs[idx], \"Image.jpg\"))\n",
    "        img = cv2.resize(img, imageSize, cv2.INTER_LINEAR)\n",
    "        maskDir=os.path.join(imgs[idx], \"Vessels\")\n",
    "        masks=[]\n",
    "        for mskName in os.listdir(maskDir):\n",
    "            vesMask = (cv2.imread(maskDir+'/'+mskName, 0) > 0).astype(np.uint8)  # Read vesse instance mask\n",
    "            vesMask=cv2.resize(vesMask,imageSize,cv2.INTER_NEAREST)\n",
    "            masks.append(vesMask)# get bounding box coordinates for each mask\n",
    "        num_objs = len(masks)\n",
    "        if num_objs==0: return loadData() # if image have no objects just load another image\n",
    "        boxes = torch.zeros([num_objs,4], dtype=torch.float32)\n",
    "        for i in range(num_objs):\n",
    "            x,y,w,h = cv2.boundingRect(masks[i])\n",
    "            boxes[i] = torch.tensor([x, y, x+w, y+h])\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "        img = torch.as_tensor(img, dtype=torch.float32)\n",
    "        data = {}\n",
    "        data[\"boxes\"] =  boxes\n",
    "        data[\"labels\"] =  torch.ones((num_objs,), dtype=torch.int64)   # there is only one class\n",
    "        data[\"masks\"] = masks\n",
    "        batch_Imgs.append(img)\n",
    "        batch_Data.append(data)  # load images and masks\n",
    "    batch_Imgs = torch.stack([torch.as_tensor(d) for d in batch_Imgs], 0)\n",
    "    batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)\n",
    "    return batch_Imgs, batch_Data\n",
    "\n",
    "model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)  # load an instance segmentation model pre-trained pre-trained on COCO\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one\n",
    "model.to(device)# move model to the right devic\n",
    "\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-5)\n",
    "model.train()\n",
    "\n",
    "for i in range(10001):\n",
    "            images, targets = loadData()\n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss_dict = model(images, targets)\n",
    "\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "            print(i,'loss:', losses.item())\n",
    "            if i%500==0:\n",
    "                torch.save(model.state_dict(), str(i)+\".torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torchvision.models.segmentation\n",
    "import torch\n",
    "imageSize=[600,600]\n",
    "imgPath=\"Image.jpg\"\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')   # train on the GPU or on the CPU, if a GPU is not available\n",
    "model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)  # load an instance segmentation model pre-trained pre-trained on COCO\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one\n",
    "model.load_state_dict(torch.load(\"10000.torch\"))\n",
    "model.to(device)# move model to the right devic\n",
    "model.eval()\n",
    "\n",
    "images=cv2.imread(imgPath)\n",
    "images = cv2.resize(images, imageSize, cv2.INTER_LINEAR)\n",
    "images = torch.as_tensor(images, dtype=torch.float32).unsqueeze(0)\n",
    "images=images.swapaxes(1, 3).swapaxes(2, 3)\n",
    "images = list(image.to(device) for image in images)\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = model(images)\n",
    "\n",
    "im= images[0].swapaxes(0, 2).swapaxes(0, 1).detach().cpu().numpy().astype(np.uint8)\n",
    "im2 = im.copy()\n",
    "for i in range(len(pred[0]['masks'])):\n",
    "    msk=pred[0]['masks'][i,0].detach().cpu().numpy()\n",
    "    scr=pred[0]['scores'][i].detach().cpu().numpy()\n",
    "    if scr>0.8 :\n",
    "        im2[:,:,0][msk>0.5] = random.randint(0,255)\n",
    "        im2[:, :, 1][msk > 0.5] = random.randint(0,255)\n",
    "        im2[:, :, 2][msk > 0.5] = random.randint(0, 255)\n",
    "cv2.imshow(str(scr), np.hstack([im,im2]))\n",
    "cv2.waitKey()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
